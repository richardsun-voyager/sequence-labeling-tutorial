{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A LSTM-CRF Tutorial \n",
    "This tutorial aims to introduce an sequence labeling model based on a LSTM-CRF architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Named-entity recognition (NER) is a sub task within information extraction, that seeks to recognize and classify named entity mentions in text into pre-defined semantic categories((e.g., location, organization, geo-political entity, person). For example, given a sentence \"Cook bought two hundred shares of Apple Inc. in 2003.\", the name entities can be highlighted as:\n",
    "\n",
    "$[Cook]_{Person} \\ bought \\ two \\ hundred \\ shares \\ of \\ [Apple\\  Inc.]_{Organization} \\ in  \\ [2003]_{Time}.$\n",
    "\n",
    "In this example, a one-token person name, a two-token company name and a temporal expression have been located and classified. \n",
    "\n",
    "NER has a wide range of applications in the industry. It can be used in recognizing relevant entities in customer complaints, which will be classified accordingly and forwarded to the appropriate department responsible for the identified product. It can also be used in search engine algorithms, to extract entities and compare them with the tags associated with the website articles for a quick and efficient search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER problem can be viewed as a sequence labeling problem, a simple class of structural prediction problems. The goal is to assign one discrete label to each member of a sequence of input tokens. We may have hearde of Hidden Markov Model (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Fields (CRF), these models can be applied on sequence labeling tasks.\n",
    "\n",
    "In this tutorial, we will implement a sequence labeling system using CRF, then evaluate it on a NER task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MIT Movie Corpus]((https://groups.csail.mit.edu/sls/downloads) is a semantically tagged training and test corpus in BIO(short for beginning, inside, outside) format. The original English corpus has been split into the training, development and test sets. You can find them under the \"data\" folder.\n",
    "\n",
    "The labeled data has the format of one token per line with label and token separated by tab, and sentences are separated by empty lines. There are three general formats of labels:\n",
    "- O: the corresponding token is outside of any name entity.\n",
    "- B-(*category*): the corresponding token is the beginning of a name entity that belongs to the category in the parentheses.\n",
    "- I-(*category*): the corresponding token is inside a name entity that belongs to the category in the parentheses.\n",
    "\n",
    "Let us look at a sample sentence in the training set:\n",
    "\n",
    "**O** did\n",
    "\n",
    "**B-DIRECTOR** sofia\n",
    "\n",
    "**I-DIRECTOR** coppola\n",
    "\n",
    "**O** direct\n",
    "\n",
    "**O** any\n",
    "\n",
    "**B-GENRE** adventure\n",
    "\n",
    "**O** films\n",
    "\n",
    "In this sample, there are two name entities \"sofia coppola\" and \"adventure\", which belong to \"DIRECTOR\" category and \"GENRE\" category respectively. The labels \"B-DIRECTOR\", \"I-DIRECTOR\" indicate the span of the entity \"sofia coppola\" whose category is \"DIRECTOR\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "### Task 1\n",
    "First of all, let us load the sentences and labels from local files. The data has been preprocessed, **don't make any changes to the labels and tokens.**\n",
    "- Write a function to read the sentences and labels given the path of a text file under the \"data\" folder.\n",
    "- Load training, development and test datasets from the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "train_path = \"data/ner_train.txt\"\n",
    "dev_path = \"data/ner_dev.txt\"\n",
    "test_path = \"data/ner_test.txt\"\n",
    "train_sent_tags = load_sentences(train_path)\n",
    "dev_sent_tags = load_sentences(dev_path)\n",
    "test_sent_tags = load_sentences(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_word_tags = [extract_word_tag(sent_tag) for sent_tag in train_sent_tags]\n",
    "dev_word_tags = [extract_word_tag(sent_tag) for sent_tag in dev_sent_tags]\n",
    "test_word_tags = [extract_word_tag(sent_tag) for sent_tag in test_sent_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sent_tags, train_sent_words = list(zip(*train_word_tags))\n",
    "dev_sent_tags, dev_sent_words = list(zip(*dev_word_tags))\n",
    "test_sent_tags, test_sent_words = list(zip(*test_word_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens and labels must be converted to corresponding indexes for subsequent tasks. The vocabulary and pre-trained embeddings have been provided, run the code below to obtain them. We'll use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local embeddings size: (7482, 100)\n",
      "Vocabulary size 7482\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "path_dict = 'data/vocab/dict.pkl'\n",
    "path_emb = 'data/vocab/local_emb.pkl'\n",
    "with open(path_dict, 'rb') as f:\n",
    "    word2id, id2word, vocab = pickle.load(f)\n",
    "with open(path_emb, 'rb') as f:\n",
    "    local_glove_emb = pickle.load(f)\n",
    "print('Local embeddings size:', local_glove_emb.shape)\n",
    "print('Vocabulary size', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two dictionaries \"word2id\", \"id2word\". \"word2id\" can map a word to its corresponding index in the embeddings, and \"id2word\" does the opposite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of \"the\":  28\n"
     ]
    }
   ],
   "source": [
    "print('The index of \"the\": ', word2id['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also dictionaries for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 28\n",
      "The index of label \"B-DIRECTOR\": 8\n"
     ]
    }
   ],
   "source": [
    "with open('data/labels/dict.pkl', 'rb') as f:\n",
    "    label2id, id2label, label_vocab = pickle.load(f)\n",
    "print('Number of unique labels:', len(label2id))\n",
    "print('The index of label \"B-DIRECTOR\":', label2id['B-DIRECTOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF\n",
    "\n",
    "CRFs are a type of discriminative undirected probabilistic graphical model. They are used to encode known relationships between observations and construct consistent interpretations and are often used for labeling or parsing of sequential data. CRFs can model complete sequences:\n",
    "\n",
    "$$p(\\mathbf{y} \\mid \\mathbf{x}) = \\frac {exp(f(\\mathbf{x}, \\mathbf{y}) \\cdot  \\theta)}{\\sum_{\\mathbf{y}^{\\prime}}exp(f(\\mathbf{x}, \\mathbf{y}^{\\prime}) \\cdot \\theta)} \\ \\ \\ \\ (1)$$\n",
    "\n",
    "Where $\\mathbf{x}$ is a complete token sequence(i.e., a sentence), $\\mathbf{y}$ is a complete label sequence, $f(\\mathbf{x}, \\mathbf{y})$ refers to feature functions and $\\theta$ refers to the model parameters. \n",
    "\n",
    "To compute the probability $p(\\mathbf{y} \\mid \\mathbf{x})$, we need to figure out: \n",
    "- Feature functions. It is very flexible to define feature functions $f(x, y)$, which can be discrete feature functions as discussed in class as well as continuous ones like neural networks. **In this project, we'll use a bi-directional LSTM to extract features from the token sequence.**\n",
    "- Calculation of the denominator. There are exponentially many possible $y^{\\prime}$ here, it is not possible to enumerate all of them. For example, given 5 unique labels, if a sentence has 10 tokens, the number of possible label combinations will be $5^{10}$. **In this project, we'll use the forward algorithm to calculate the denominator efficiently.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "The bidirectional LSTM (biLSTM) class has been provided to extract features of sentences. The input is two Pytorch tensors, one is a batch of token index sequences and the other is the lengths of sequences in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from lstm_encoder import LSTMEncoder\n",
    "torch.manual_seed(111)\n",
    "\n",
    "#Definition of example hyper-parameters\n",
    "example_vocab_size = 100#example vocabulary size\n",
    "example_emb_dim = 32#example word embedding dimension\n",
    "example_hidden_dim = 32#example hidden dimension of LSTM\n",
    "example_batch_size = 3#example batch size\n",
    "example_max_seq_len = 10#example max length of sequences\n",
    "example_label_size = 5#number of unique labels\n",
    "#Example token sequences\n",
    "example_sentences = torch.empty(example_batch_size, \n",
    "                                example_max_seq_len).random_(example_vocab_size).long()\n",
    "example_seq_lens = torch.LongTensor([10, 7, 9])#example length of each sequence in the batch\n",
    "example_labels = torch.empty(example_batch_size, \n",
    "                                example_max_seq_len).random_(example_label_size-2).long()\n",
    "\n",
    "#Create a biLSTM object\n",
    "lstm = LSTMEncoder(example_vocab_size, example_emb_dim, example_hidden_dim)\n",
    "sents_lstm_outputs = lstm(example_sentences, example_seq_lens)\n",
    "print(sents_lstm_outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the lstm outputs to label space for CRF\n",
    "torch.manual_seed(111)\n",
    "hidden2label_spcae = nn.Linear(example_hidden_dim, example_label_size)\n",
    "example_sents_feats = hidden2label_spcae(sents_lstm_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sents_feats.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dictionaries that we obtained in Task 1, let us do such work to extract features of sentences:\n",
    "-  Write a function that can sample a batch of labeled sentences randomly from the training set, or a batch of labeled sentences sequentially from the development (test) set.\n",
    "- This function returns three tensors: a tensor of token index sequences, a tensor of label index sequences and a tensor of sequence lengths. Pad the token, label sequences in a batch to the same length as we did in homework 1.\n",
    "- Use this function to generate a batch of data from the training set, feed the data to the biLSTM encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def sent2ids(text):\n",
    "    return [word2id[w] for w in text]\n",
    "\n",
    "def labels2ids(labels):\n",
    "    return [label2id[w] for w in labels]\n",
    "\n",
    "\n",
    "def data_generator(sents, labels, batch_size=32, is_training=True, index=0):\n",
    "    if is_training:\n",
    "        select_indices = np.random.choice(len(sents), batch_size, replace=False)\n",
    "    else:\n",
    "        start = index\n",
    "        end = min(start + batch_size, len(sents)) \n",
    "        select_indices = list(range(start, end))\n",
    "    #select_indices = list(range(batch_size))\n",
    "    batch_sents = np.array(sents)[select_indices]\n",
    "    batch_labels = np.array(labels)[select_indices]\n",
    "    \n",
    "    batch_sents = list(map(sent2ids, batch_sents))\n",
    "    batch_labels = list(map(labels2ids, batch_labels))\n",
    "    \n",
    "    seq_lens = [len(s) for s in batch_sents]\n",
    "    seq_lens = torch.LongTensor(seq_lens)\n",
    "    max_len = max(seq_lens)\n",
    "    \n",
    "    batch_sents = [torch.LongTensor(s) for s in batch_sents]\n",
    "    \n",
    "    \n",
    "    batch_sents = pad_sequence(batch_sents, batch_first=True)\n",
    "    \n",
    "    if not is_training:\n",
    "        return batch_sents, batch_labels, seq_lens, end\n",
    "    batch_labels = [torch.LongTensor(s) for s in batch_labels]\n",
    "    batch_labels = pad_sequence(batch_labels, batch_first=True)\n",
    "    \n",
    "    return batch_sents, batch_labels, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_sents, batch_label, seq_lens = data_generator(train_sent_words, train_sent_tags, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Let us look at the numerator of Formula 1, we can use score to represent the output of feature functions:\n",
    "\n",
    "$$score(\\mathbf{x}, \\mathbf{y})=f(\\mathbf{x}, \\mathbf{y}) \\cdot  \\theta \\ \\ \\ \\ (2)$$\n",
    "\n",
    "![crf](imgs/crf2.png)\n",
    "<center>Figure 1\n",
    "\n",
    "We'll focus on a linear-chain CRF model that only considers the transitions of two consecutive labels. For any given label sequence, we can connect them one by one and make a path from the start to the end. Take Figure 1 for example, assume the label sequence is \"A V P D N\", the path is shown as red bold arrows. There's a score for each path in CRF, a good path has a relatively large score. **Note that each path starts from a START label and ends with a STOP label.**\n",
    "\n",
    "In a linear-chain CRF, the score can be computed as a sum of log potentials:\n",
    "\n",
    "$$score(\\mathbf{x}, \\mathbf{y}) = \\sum_{j=1}^{n} log\\phi(\\mathbf{x}, y_{j-1}, y_j, j) + A_{y_{n}, y_{n+1}} \\ \\ \\ \\ (3)$$\n",
    "\n",
    "Where $y_{j-1}$, $y_j$ are the labels at position $j-1$ and $j$ respectively, $n$ is the sequence length, $A \\in R^{l \\times l}$ is the transition matrix, $l$ is the unique label number, and\n",
    "$y_{0}=START$ and $y_{n+1} = STOP$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Similar to HMM, a potential can be denoted as the product of **an emission potential** and **a transition potential**, let us write them in the log form:\n",
    "\n",
    "$$log\\phi(\\mathbf{x}, y_{j-1}, y_j, j) = log \\phi_{emit}(x_j, y_j) +  log \\phi_{trans}(y_{j-1}, y_j) \\ \\ \\ \\ (4)$$\n",
    "\n",
    "$$log \\phi_{emit}(y_j, x_j) = (W_xh_j)_{y_j} \\ \\ \\ \\ (5)$$\n",
    "\n",
    "$$log \\phi_{trans}(y_{j-1}, y_j) = A_{y_{j-1}, y_j} \\ \\ \\ \\ (6) $$\n",
    "\n",
    "Where $h_j \\in R^d$ is the biLSTM output of the token sequence at position $j$, $W_x \\in R^{l\\times d}$ refers to parameters that map biLSTM outputs to label space, $d$ is the hidden dimension of biLSTM.\n",
    "\n",
    "With Formula (4), (5) and (6), we can obtain:\n",
    "\n",
    "$$log\\phi(\\mathbf{x}, y_{j-1}, y_j, j) = (W_xh_j)_{y_j} + A_{y_{j-1}, y_j} \\ \\ \\ \\ (7)$$\n",
    "\n",
    "**The log emission potentials** can be obtained from the biLSTM output, and **the log transmission potentials can be obtained from the transition matrix.** Therefore, we can compute each potential based on the features extracted by biLSTM and the transition matrix. The transition matrix is viewed as parameters that will be randomly initialized and learned on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearChainCRF(nn.Module):\n",
    "    def __init__(self, label_size):\n",
    "        super(LinearChainCRF, self).__init__()\n",
    "        \n",
    "        self.label_size = label_size\n",
    "        self.start_label = label_size - 1\n",
    "        self.stop_label = label_size - 2\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.label_size, self.label_size))\n",
    "        self.transitions.data[:, self.start_label] = -10000\n",
    "        self.transitions.data[self.stop_label] = -10000\n",
    "     \n",
    "    def forward_alg(self, features):\n",
    "        '''\n",
    "        Forward algorithm\n",
    "        Arg:\n",
    "            features: features of a sentence, sent_len*label_size, tensor\n",
    "        Return:\n",
    "            alpha: the log sum of \n",
    "        '''\n",
    "        #Complete the code\n",
    "        init_alphas = torch.full((1, self.label_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0, self.start_label] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in features:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.label_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.label_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[:, next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[:, self.stop_label]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "    def sentence_scorer(self, features, labels):\n",
    "        '''\n",
    "        Score a sentence given its label sequence\n",
    "        Args:\n",
    "            features: features of a sentence, sent_len*label_size, tensor\n",
    "            labels: a sequence of labels, sent_len, tensor\n",
    "        Return:\n",
    "            score: the score of the labeled sentence, a scalar\n",
    "        '''\n",
    "        #Complete the code\n",
    "        score = torch.zeros(1)\n",
    "        labels = torch.cat([torch.tensor([self.start_label], dtype=torch.long), labels])\n",
    "        for i, feat in enumerate(features):\n",
    "            score = score + \\\n",
    "                self.transitions[labels[i], labels[i + 1]] + feat[labels[i + 1]]\n",
    "        score = score + self.transitions[labels[-1], self.stop_label]\n",
    "        return score\n",
    "    \n",
    "    def viterbi_decoder(self, features):\n",
    "        '''\n",
    "        Viterbi decoder\n",
    "        Arg:\n",
    "            features: features of a sentence, sent_len*label_size, tensor\n",
    "        Return:\n",
    "            best_path_score: best path score, a scalar\n",
    "            best_path: best path, a list\n",
    "        '''\n",
    "        #Complete the code\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.label_size), -10000.)\n",
    "        init_vvars[0][self.start_label] = 0\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in features:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.label_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[:, next_tag].view(1, -1)\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[:, self.stop_label].view(1, -1)\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        best_path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.start_label  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return best_path_score, best_path\n",
    "    \n",
    "\n",
    "def argmax(x):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(x, 1)\n",
    "    return idx.item()\n",
    "\n",
    "    \n",
    "def log_sum_exp(x):\n",
    "    #return log sum exp of a tensor\n",
    "    m = torch.max(x, -1)[0]\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(-1)), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7266], grad_fn=<AddBackward0>) tensor([11.1678], grad_fn=<AddBackward0>)\n",
      "tensor(6.1943, grad_fn=<SelectBackward>) [1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "model = LinearChainCRF(example_label_size)\n",
    "score = model.sentence_scorer(example_sents_feats[0], example_labels[0])\n",
    "alpha = model.forward_alg(example_sents_feats[0])\n",
    "path_score, best_path = model.viterbi_decoder(example_sents_feats[0])\n",
    "print(score, alpha)\n",
    "print(path_score, best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7266], grad_fn=<AddBackward0>) tensor([11.1678], grad_fn=<AddBackward0>)\n",
      "tensor(6.1943, grad_fn=<SelectBackward>) [1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "model2 = LinearChainCRF2(example_label_size)\n",
    "model2.transitions.data = model.transitions.data.transpose(1, 0)\n",
    "score = model2._score_sentence(example_sents_feats[0], example_labels[0])\n",
    "alpha = model2._forward_alg(example_sents_feats[0])\n",
    "path_score, best_path = model2._viterbi_decode(example_sents_feats[0])\n",
    "print(score, alpha)\n",
    "print(path_score, best_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear-chain CRF class has been created, let us complete such work:\n",
    "- Complete the function \"sentence_scorer\" that can compute the score given features of a sentence and a label sequence.\n",
    "- Run your function on \"example_sents_feats\" and \"example_labels\".\n",
    "\n",
    "You can use pre-defined functions \"argmax\" and \"log_sum_exp\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Using Formula (3) we can compute the score given a sentence features and a label sequence. To obtain the probability of a sequence, we still need to compute the denominator (the sum of all possible exponential values of scores). Let us use log probability instead of probability for convenience:\n",
    "\n",
    "$$logp(\\mathbf{y} \\mid \\mathbf{x}) =  score(\\mathbf{x}, \\mathbf{y}) - log(\\sum_{\\mathbf{y}^{\\prime}}exp(score(\\mathbf{x}, \\mathbf{y}^{\\prime})))  \\ \\ \\ \\ (8)$$\n",
    "\n",
    "We'll use the forward algorithm to calculate the log denominator efficiently.\n",
    "- Complete the function \"forward_alg\" in the class \"LinearChainCRF\". Consider the START and the STOP label in your algorithm.\n",
    "- Run your function on \"example_sents_feats\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.1678], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = model.forward_alg(example_sents_feats[0])\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Now we have functions to compute both the score of a label sequence and the log denominator, the log probability can be obtained using Formula (9):\n",
    "- Compute the log probability of the sequence \"example_labels\" given the sentence features \"example_sents_feats\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-13.8944], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score - alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn parameters\n",
    "\n",
    "Up to now, we haven't updated any parameters. A good CRF model can learn from the training set and give high probabilities to the gold label sequences by updating the parameters. The loss can be denoted as the negative sum of log probabilities:\n",
    "\n",
    "$$ -\\sum_i^m logp(\\mathbf{y_i} \\mid \\mathbf{x_i}) \\ \\ \\ \\ (9)$$\n",
    "\n",
    "Where $m$ is the number of sentences. The higher the probabilities of gold label sequences, the smaller the loss. Fortunately, we don't need to compute the gradients of parameters as Pytorch will do that automatically.\n",
    "\n",
    "### Task 6\n",
    "A \"SequenceLabeling\" class has been created below to train a CRF model, let us do such work:\n",
    "- Complete the function \"forward\"  that compute the loss of a batch of training data.\n",
    "- Complete the function \"load_pretrained_emb\" that can load pre-trained Glove embeddings to biLSTM object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequenceLabeling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, path):\n",
    "        super(SequenceLabeling, self).__init__()\n",
    "        self.crf = LinearChainCRF(label_size)\n",
    "        self.bilstm = LSTMEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "        self.hidden2label_space = nn.Linear(hidden_dim, label_size)\n",
    "        self.load_pretrained_emb(path)\n",
    "        \n",
    "        \n",
    "    def forward(self, sentences, seq_lens, sent_tags):\n",
    "        '''\n",
    "        Compute the loss\n",
    "        Args:\n",
    "            sentences: batch_size*word_num, long tensor\n",
    "            seq_lens: batch_size, long tensor\n",
    "            sent_tags: batch_size*word_num, long tensor\n",
    "        Return:\n",
    "            loss: the average loss of a batch\n",
    "        '''\n",
    "        hiddens = self.bilstm(sentences, seq_lens)\n",
    "        #batch_size*word_num*tag_size\n",
    "        batch_feats = self.hidden2label_space(hiddens)\n",
    "        neg_likelihoods = []\n",
    "        for i, feats in enumerate(batch_feats):\n",
    "            length = seq_lens[i]\n",
    "            tags = sent_tags[i]\n",
    "            gold_score = self.crf.sentence_scorer(feats[:length], tags[:length])\n",
    "            forward_score = self.crf.forward_alg(feats[:length])\n",
    "            neg_likelihoods.append(forward_score-gold_score)\n",
    "        loss = torch.stack(neg_likelihoods).mean()\n",
    "        return loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def decoder(self, sentences, seq_lens):\n",
    "        '''\n",
    "        Viterbi decoder\n",
    "        Args:\n",
    "            sentences: batch_size*word_num, long tensor\n",
    "            seq_lens: batch_size, long tensor\n",
    "        Return:\n",
    "            pred_path: batch_size, list\n",
    "        '''\n",
    "        hiddens = self.bilstm(sentences, seq_lens)\n",
    "        #batch_size*word_num*tag_size\n",
    "        batch_feats = self.hidden2label_space(hiddens)\n",
    "        preds = []\n",
    "        for i, feats in enumerate(batch_feats):\n",
    "            length = seq_lens[i]\n",
    "            _, pred = self.crf.viterbi_decoder(feats[:length])\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "\n",
    "        return pred_path\n",
    "    \n",
    "    def load_pretrained_emb(self, path):\n",
    "        '''\n",
    "        Load pre-trained word embeddings from the path\n",
    "        Arg:\n",
    "            path: the binary file of local Glove embeddings\n",
    "        '''\n",
    "        with open(path, 'rb') as f:\n",
    "            vectors = pickle.load(f)\n",
    "            print(\"Loaded from {} with shape {}\".format(path, vectors.shape))\n",
    "            assert vectors.shape == self.bilstm.word_embeds.weight.size()\n",
    "            self.bilstm.word_embeds.weight.data.copy_(torch.from_numpy(vectors))\n",
    "            self.bilstm.word_embeds.weight.requires_grad = False\n",
    "            print('embeddings loaded')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance\n",
    "\n",
    "Similar to constituency-parsing, the evaluation of a NER task is to match the predicted sequence and the gold sequence. We need to generate an optimal label sequence given a sentence. It is impossible to enumerate all of the paths (label sequences) and choose the one with the largest score.\n",
    "\n",
    "### Task 7\n",
    "We can use the Viterbi algorithm in choosing the optimal label sequence.\n",
    "- Complete the function \"viterbi_decoder\" in the class \"LinearChainCRF\".\n",
    "- Complete the function \"decoder\" in the class \"SequenceLabeling\".\n",
    "- Write a function that converts the predicted label index sequences to label sequences. For example, [0, 0, 3, 4, 0, 0] -> ['O', 'O', 'B-ACTOR', 'I-ACTOR', 'O', 'O']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/vocab/local_emb.pkl with shape (7482, 100)\n",
      "embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "model = SequenceLabeling(len(vocab), 100, 64, len(label2id), 'data/vocab/local_emb.pkl')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 32.355308532714844\n",
      "Loss: 31.648584365844727\n",
      "Loss: 18.129901885986328\n",
      "Loss: 19.353574752807617\n",
      "Loss: 15.430071830749512\n",
      "Loss: 16.556926727294922\n",
      "Loss: 14.95309829711914\n",
      "Loss: 15.039655685424805\n",
      "Loss: 18.279136657714844\n",
      "Loss: 12.75913143157959\n",
      "Loss: 11.560237884521484\n",
      "Loss: 9.767963409423828\n",
      "Loss: 10.669052124023438\n",
      "Loss: 12.229289054870605\n",
      "Loss: 8.61056900024414\n",
      "Loss: 7.509668350219727\n",
      "Loss: 10.254934310913086\n",
      "Loss: 8.366771697998047\n",
      "Loss: 7.92102575302124\n",
      "Loss: 6.970520973205566\n",
      "Loss: 9.297639846801758\n",
      "Loss: 5.751221656799316\n",
      "Loss: 7.595761775970459\n",
      "Loss: 7.319451808929443\n",
      "Loss: 7.221586227416992\n",
      "Loss: 8.557435989379883\n",
      "Loss: 7.179542541503906\n",
      "Loss: 6.736079216003418\n",
      "Loss: 8.453967094421387\n",
      "Loss: 8.435871124267578\n",
      "Loss: 6.522689342498779\n",
      "Loss: 6.860531806945801\n",
      "Loss: 8.072932243347168\n",
      "Loss: 6.386556625366211\n",
      "Loss: 6.640663146972656\n",
      "Loss: 6.838127136230469\n",
      "Loss: 7.408633232116699\n",
      "Loss: 7.719769477844238\n",
      "Loss: 6.230581283569336\n",
      "Loss: 5.880599498748779\n",
      "Loss: 5.524681568145752\n",
      "Loss: 5.110983848571777\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "epoch = 3\n",
    "loop_num = int(len(train_sent_words)/32)\n",
    "for i in range(epoch):\n",
    "    start = time()\n",
    "    model.train()\n",
    "    for j in range(loop_num):\n",
    "        optimizer.zero_grad()\n",
    "        batch_sents, batch_tags, seq_lens = data_generator(train_sent_words,train_sent_tags)\n",
    "        loss = model(batch_sents, seq_lens, batch_tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 20 == 0:\n",
    "            print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Like what we did in homework 2, before performing evaluation, we need to convert the label sequences to name entity brackets. A name entity is denoted as $E(i, j)$ where $E$ is the category, $(i,j)$ refer to the span that the entity covers ($i \\leq j$).\n",
    "Metrics are defined as:\n",
    "- Precision = (# of correct entities in the predicted sequences)/(# of total entities in the predicted sequences)\n",
    "\n",
    "- Recall = (# of correct entities in the predicted sequences)/(# of correct entities in the gold sequences)\n",
    "\n",
    "- F1 score = 2\\*precision\\*recall/(precision+recall)\n",
    "\n",
    "Let us see the example in the part of Dataset again.\n",
    "\n",
    "**O** did\n",
    "\n",
    "**B-DIRECTOR** sofia\n",
    "\n",
    "**I-DIRECTOR** coppola\n",
    "\n",
    "**O** direct\n",
    "\n",
    "**O** any\n",
    "\n",
    "**B-GENRE** adventure\n",
    "\n",
    "**O** films\n",
    "\n",
    "The gold name entities can be represented as: $DIRECTOR(1, 2)$, $GENRE(5, 5)$, assume the predicted name entities as $DIRECTOR(1, 2)$, $GENRE(5, 5)$, $TITLE(6, 6)$, then the precision=2/3, the recall=2/2, and the F1 score=0.8.\n",
    "\n",
    "\n",
    "Let us work as following:\n",
    "- Write a function to do the convertion.\n",
    "- Write a function to compute the metrics given the predicted label sequences and gold label sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Span(a, b, c):\n",
    "    return tuple((a, b, c))\n",
    "\n",
    "def evaluate(real_tags, pred_tags):\n",
    "    '''\n",
    "    Evaluate based on the brackets including the label and the positions\n",
    "    '''\n",
    "    p = 0\n",
    "    num = len(real_tags)\n",
    "    total_entity = 0\n",
    "    total_predict = 0\n",
    "\n",
    "    for i in range(num):\n",
    "\n",
    "        output = real_tags[i]\n",
    "        prediction = pred_tags[i]\n",
    "        #convert to span\n",
    "        output_spans = set()\n",
    "        start = -1\n",
    "        previous = ' '\n",
    "        for i in range(len(output)):\n",
    "            if output[i].startswith(\"B-\"):\n",
    "                if previous.startswith(\"B-\"):\n",
    "                    output_spans.add(Span(i-1, i-1, previous[2:]))\n",
    "                if previous.startswith(\"I-\"):\n",
    "                    output_spans.add(Span(start, i-1, previous[2:]))\n",
    "                if i == (len(output)-1):#The last one begins with B\n",
    "                    output_spans.add(Span(i, i, output[i][2:]))\n",
    "                start = i\n",
    "            if output[i].startswith(\"I-\"):\n",
    "                if i == (len(output)-1):#The last one begins with I\n",
    "                    output_spans.add(Span(start, i, output[i][2:]))\n",
    "            if output[i].startswith(\"O\"):\n",
    "                if previous.startswith(\"B-\"):\n",
    "                    output_spans.add(Span(i-1, i-1, previous[2:]))\n",
    "                if previous.startswith(\"I-\"):\n",
    "                    output_spans.add(Span(start, i-1, previous[2:]))   \n",
    "            previous = output[i]\n",
    "        #print('Output spans:', output_spans)\n",
    "        predict_spans = set()\n",
    "        start = -1\n",
    "        previous = ' '\n",
    "        for i in range(len(prediction)):\n",
    "            if prediction[i].startswith(\"B-\"):\n",
    "                if previous.startswith(\"B-\"):\n",
    "                    predict_spans.add(Span(i-1, i-1, previous[2:]))\n",
    "                if previous.startswith(\"I-\"):\n",
    "                    predict_spans.add(Span(start, i-1, previous[2:]))\n",
    "                if i == (len(output)-1):#The last one begins with B\n",
    "                    predict_spans.add(Span(i, i, output[i][2:]))\n",
    "                start = i\n",
    "            if prediction[i].startswith(\"I-\"):\n",
    "                if i == (len(output)-1):#The last one begins with I\n",
    "                    predict_spans.add(Span(start, i, prediction[i][2:]))\n",
    "            if prediction[i].startswith(\"O\"):\n",
    "                if previous.startswith(\"B-\"):\n",
    "                    predict_spans.add(Span(i-1, i-1, previous[2:]))\n",
    "                if previous.startswith(\"I-\"):\n",
    "                    predict_spans.add(Span(start, i-1, previous[2:]))   \n",
    "            previous = prediction[i]\n",
    "        \n",
    "        #print('Predict spans:', predict_spans)\n",
    "\n",
    "        total_entity += len(output_spans)\n",
    "        total_predict += len(predict_spans)\n",
    "        p += len(predict_spans.intersection(output_spans))\n",
    "    \n",
    "    print(p, total_entity, total_predict)\n",
    "    precision = p * 1.0 / total_predict * 100 if total_predict != 0 else 0\n",
    "    recall = p * 1.0 / total_entity * 100 if total_entity != 0 else 0\n",
    "    fscore = 2.0 * precision * recall / (precision + recall) if precision != 0 or recall != 0 else 0\n",
    "\n",
    "    return [precision, recall, fscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "index = 0\n",
    "pred_label_list = []\n",
    "while index < len(dev_sent_words):\n",
    "    batch_sents, batch_tags, seq_lens, index = data_generator(dev_sent_words, \n",
    "                                                              dev_sent_tags, batch_size=32, \n",
    "                                                              is_training=False, index=index)\n",
    "    pred_labels = model.decoder(batch_sents, seq_lens)\n",
    "    for label_seq in pred_labels:\n",
    "        pred_labels = [id2label[t] for t in label_seq]\n",
    "        pred_label_list.append(pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Finally, we have built a sequence labeling system step by step. Let us explore the power of it on our NER task:\n",
    "- Tune hyperparameters on the development set based on the F1 score using the pre-trained Glove word embeddings.\n",
    "- Evaluate the performance on the test set, report the metrics.\n",
    "- Choose 5 sentences in the test set randomly, display the predicted label sequences as well as the gold label sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_label_list = []\n",
    "index = 0\n",
    "while index < len(test_sent_words):\n",
    "    batch_sents, batch_tags, seq_lens, index = data_generator(test_sent_words, \n",
    "                                                              test_sent_tags, batch_size=32, \n",
    "                                                              is_training=False, index=index)\n",
    "    pred_labels = model.decoder(batch_sents, seq_lens)\n",
    "    for label_seq in pred_labels:\n",
    "        pred_labels = [id2label[t] for t in label_seq]\n",
    "        pred_label_list.append(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3804 5517 5339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[71.24929762127739, 68.9505165851006, 70.0810611643331]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(pred_label_list, test_sent_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative experiment\n",
    "\n",
    "### Task 10 \n",
    "Our current model is biLSTM+CRF, you may wonder how much the CRF can contribute to the NER task. Let us make a comparative experiment:\n",
    "- Design a simple biLSTM label sequencing system that can predict the labels directly based on the biLSTM outputs.\n",
    "- Train your biLSTM system, tune hyper-parameters and evaluate it on the test set.\n",
    "- Run both biLSTM+CRF and biLSTM models several times, and analyse whether biLSTM+CRF outperforms biLSTM based on the results or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can view this sequence labeling problem as a classification problem, namely, to label each word with its hidden output from LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleSequenceLabeling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, path):\n",
    "        super(SimpleSequenceLabeling, self).__init__()\n",
    "        self.crf = LinearChainCRF(label_size)\n",
    "        self.bilstm = LSTMEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "        self.hidden2label_space = nn.Linear(hidden_dim, label_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.loss = nn.NLLLoss()\n",
    "        self.load_pretrained_emb(path)\n",
    "        \n",
    "        \n",
    "    def forward(self, sentences, seq_lens):\n",
    "        '''\n",
    "        Compute the loss\n",
    "        Args:\n",
    "            sentences: batch_size*word_num, long tensor\n",
    "            seq_lens: batch_size, long tensor\n",
    "        Return:\n",
    "            out: log softmax output, float tensor\n",
    "        '''\n",
    "        hiddens = self.bilstm(sentences, seq_lens)\n",
    "        #batch_size*word_num*hidden_size\n",
    "        outputs = []\n",
    "        for i, l in enumerate(seq_lens):\n",
    "            out = self.hidden2label_space(hiddens[i, :l])\n",
    "            out = self.softmax(out)\n",
    "            out = torch.log(out)\n",
    "            outputs.append(out)\n",
    "        return outputs\n",
    "    \n",
    "    def pred_seq(self, seqs, seq_lens):\n",
    "        outputs = self.forward(seqs, seq_lens)\n",
    "        preds = []\n",
    "        for out in outputs:\n",
    "            pred = out.argmax(1)\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "    def get_seq_loss(self, seqs, seq_lens, tag_seqs):\n",
    "        '''\n",
    "        Compute the loss\n",
    "        Args:\n",
    "            seqs: batch_size*word_num, long tensor\n",
    "            seq_lens: batch_size, long tensor\n",
    "            tag_seqs: batch_size*word_num, long tensor\n",
    "        Return:\n",
    "            loss: loss, float\n",
    "        '''\n",
    "        outputs = self.forward(seqs, seq_lens)\n",
    "        losses = []\n",
    "        for i, out in enumerate(outputs):\n",
    "            loss = self.loss(out, tag_seqs[i, :seq_lens[i]])\n",
    "            loss = loss * seq_lens[i]\n",
    "            losses.append(loss)\n",
    "        return torch.stack(losses).sum()/seq_lens.sum()\n",
    "\n",
    "    \n",
    "    def load_pretrained_emb(self, path):\n",
    "        '''\n",
    "        Load pre-trained word embeddings from the path\n",
    "        Arg:\n",
    "            path: the binary file of local Glove embeddings\n",
    "        '''\n",
    "        with open(path, 'rb') as f:\n",
    "            vectors = pickle.load(f)\n",
    "            print(\"Loaded from {} with shape {}\".format(path, vectors.shape))\n",
    "            assert vectors.shape == self.bilstm.word_embeds.weight.size()\n",
    "            self.bilstm.word_embeds.weight.data.copy_(torch.from_numpy(vectors))\n",
    "            self.bilstm.word_embeds.weight.requires_grad = False\n",
    "            print('embeddings loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/vocab/local_emb.pkl with shape (7482, 100)\n",
      "embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "model = SimpleSequenceLabeling(len(vocab), 100, 64, len(label2id), 'data/vocab/local_emb.pkl')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4073823392391205\n",
      "Loss: 0.4119170606136322\n",
      "Loss: 0.5569366812705994\n",
      "Loss: 0.5293269157409668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-517ff3c10b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sent_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sent_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seq_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-49d05aaa1cad>\u001b[0m in \u001b[0;36mget_seq_loss\u001b[0;34m(self, seqs, seq_lens, tag_seqs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         '''\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-49d05aaa1cad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences, seq_lens)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlog\u001b[0m \u001b[0msoftmax\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         '''\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m#batch_size*word_num*hidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/Dropbox/HW/NLP_course/assignment_drafts/final project/lstm_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences, seq_lens)\u001b[0m\n\u001b[1;32m     43\u001b[0m                                                  perm_seq_lens, batch_first=True)\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#Restore the order of sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_packed\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[0;32m--> 525\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "epoch = 3\n",
    "loop_num = int(len(train_sent_words)/32)\n",
    "loss_func = nn.NLLLoss()\n",
    "for i in range(epoch):\n",
    "    start = time()\n",
    "    model.train()\n",
    "    for j in range(loop_num):\n",
    "        optimizer.zero_grad()\n",
    "        batch_sents, batch_tags, seq_lens = data_generator(train_sent_words, train_sent_tags)\n",
    "        loss = model.get_seq_loss(batch_sents, seq_lens, batch_tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 50 == 0:\n",
    "            print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_label_list = []\n",
    "index = 0\n",
    "while index < len(test_sent_words):\n",
    "    batch_sents, batch_tags, seq_lens, index = data_generator(test_sent_words, \n",
    "                                                              test_sent_tags, batch_size=32, \n",
    "                                                              is_training=False, index=index)\n",
    "    pred_labels = model.pred_seq(batch_sents, seq_lens)\n",
    "    for label_seq in pred_labels:\n",
    "        pred_labels = [id2label[t.item()] for t in label_seq]\n",
    "        pred_label_list.append(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3552 5421 5339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[66.52931260535681, 65.5229662423907, 66.02230483271374]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(pred_label_list, test_sent_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Challenge (optional)\n",
    "\n",
    "- Now, think of a better design for developing improved sequence labeling systems using any model you like.\n",
    "- Implement your model and run experiments on the NER datasets.\n",
    "- Write a report to elaborate your model, analyse the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free GPU Resources\n",
    "We suggest you run neural models on machines with GPU(s). Google provides a free online platform [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb), a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use as common packages have been  pre-installed. Google users can have access to a Tesla T4 GPU (approximately 15G memory). Note that when you connect to a GPU-based VM runtime, you are given a maximum of 12 hours at a time on the VM.\n",
    "\n",
    "It is convenient to upload local Jupyter Notebook files and data to Colab, please refer to the [tutorial](https://colab.research.google.com/notebooks/io.ipynb). \n",
    "\n",
    "Microsoft also provides online platform [Azure Notebooks](https://notebooks.azure.com/help/introduction) for data science and machine learning, there are free versions as well as free trials for new users with credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
